---
title: "Practical Machine Learning Project"
author: "Everett Robinson"
date: "May 23, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(doParallel)
set.seed(73222)
```

## Data Exploration and Cleaning
We start by importing the data from the provided csv files. I am going to split the training data into a new training subset and a cross validation subset and use the provided pml-testing.csv file for final model testing.
```{r import}
raw_pml_training <- read.csv("pml-training.csv")
in_training <- createDataPartition(y = raw_pml_training$classe, p = 0.8, list = FALSE)
training <- raw_pml_training[in_training,]
validation <- raw_pml_training[-in_training,]
testing <- read.csv("pml-testing.csv")

dim(training)
dim(validation)
```

It looks like there are a large number of rows of data to be used for training and validation, so I will stick with this simple data partition and not use any folding or resampling techniques on the data. There are a large number of predictors however so we will need to investigate these further.

```{r col_names}
names(training)
```

From the source of the data <http://groupware.les.inf.puc-rio.br/har>, we know that six different participants were asked to complete dumbbell bicep curls in five different ways, the first using proper technique and the next four using a variety of incorrect techniques. These differing techniques are identified in the classe variable using the letters A through E. We also know that the main components of the data come from four separate three axis accelerometers. One accelerometer was mounted on the dumbbell, and three were located on the participants waist, forearm, and upper arm.

The first seven columns appear to be related to data collection rather than actual measurements, and they will need to be removed from each data frame before training is performed:
```{r}
head(training[1:7])
```

```{r}
non_measurement_columns <- 1:ncol(training) <= 7
```


Each accelerometer also recorded 38 different variables during each repetition:
```{r}
length(grep("dumbbell", names(training)))
length(grep("belt", names(training)))
length(grep("forearm", names(training)))
length(grep("_arm", names(training)))
```

These measurements are on various parameters such as roll, pitch, yaw, acceleration, gyros, and magnetic measurements. They are also supplemented by statistics for each measurement such as mean, minimum, maximum, amplitude, variance, standard deviation, skew, and kurtosis.

We hope that the combinations of all of the above will be sufficiently different between the different classe exercise techniques.

```{r}
ggplot(training) + geom_point(aes(x = X, y = roll_dumbbell, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```


```{r}
ggplot(training) + geom_point(aes(x = X, y = pitch_dumbbell, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```


```{r}
ggplot(training) + geom_point(aes(x = X, y = accel_dumbbell_y, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```

It appears there is a lot of variation between how the different participants perform the dumbbell curls, which reinforces the decision to remove the user_name column from the training data before training occurs to prevent it from playing a roll in the classification process. There are some visible differences between the classe groups as well, which suggests that the combination of multiple different accelerometer readings for each recorded exercise might be able to distinguish the techniques.

It is unlikely that all 38 columns from each accelerometer will be necessary to properly classify each exercise, and so we will remove or consolidate columns that don't offer a useful amount crucial to classification.

The first step in this process will be to figure out which columns are not well populated with data. After running **View(training)**, the columns containing the summary statistics appear to be very sparse. I suspect that it is the case that only the last row of data in each num_window set was populated with these values, and now they are spread across our training, validation, and testing data sets. Because it would be difficult to correct this issue while still maintaining the separation of the datasets, these columns are prime candidates for removal.

We will start by figuring out how sparse each column actually is:

```{r}
percent_sparse <- function(col) {
  sum((is.na(col) | col == "")/ length(col))
}

sparseness <- apply(training, 2, percent_sparse)

table(sparseness)
names(sparseness[sparseness > 0.95])
```

So it turns out that all of the columns containing summary statistics are nearly 98% empty. We will remove these when cleaning the data:
```{r}
sparse_columns <- sparseness > 0.95
```


The above cleaning steps will need to be performed before the data is ready to be used for training. It is necessary that the same preprocessing steps be performed on each of the training, validation, and testing data frames, so I will put all of the steps in a function that can be run on all three data frames to ensure consistency.
```{r}
clean_data <- function(data) {
  drop_columns <- non_measurement_columns | sparse_columns
  data <- data[,!drop_columns]
  return(data)
}

clean_training <- clean_data(training)
clean_validation <- clean_data(validation)
clean_testing <- clean_data(testing)
```



## Model Building and Selection

We will train several different types of machine learning models before evaluating them both individually and stacked with each other. We will test linear discriminant analysis, random forests, gradient boosting, and support vector machines. The stacked model will be combined using random forests on the predictions of the previous four models. All of these models will be trained using the default parameters provided through the caret package.

```{r}
set.seed(82917)

# We will set up a cluster to parallelize the training steps for performance reasons
cl <- makeCluster(6)
registerDoParallel(cl)
```

```{r linear_discriminant_analysis}
lda_mod <- train(classe ~ ., method = "lda", data = clean_training)
lda_pred <- predict(lda_mod, clean_validation)
```

```{r random_forest}
rf_mod <- train(classe ~ ., method = "parRF", data = clean_training)
rf_pred <-predict(rf_mod, clean_validation)
```

```{r gradient_boosting}
xgbTree_mod <- train(classe ~ ., method = "xgbTree", data = clean_training)
xgbTree_pred <- predict(xgbTree_mod, clean_validation)
```


```{r support_vector_machines}
svm_mod <- train(classe ~ ., method = "svmLinear", data = clean_training)
svm_pred <- predict(svm_mod, clean_validation)
```

```{r stacked}
stacked <- data.frame(lda_pred, rf_pred, xgbTree_pred, svm_pred, classe = clean_validation$classe)
stacked_mod <- train(classe ~ ., method = "rf", data = stacked)
stacked_pred <- predict(stacked_mod, clean_validation)
```

And with all of our models trained we can now see how well each of them performed using the confusionMatrix function in the caret package.

###Linear Discriminant Performance:
```{r}
lda_validation <- confusionMatrix(lda_pred, clean_validation$classe)
lda_validation
```
Linear Discriminant Analysis using the default settings provided by caret performed better than random guessing, but with a cross validated accuracy of `r lda_validation$overall[["Accuracy"]]` we can only expect to get all 20 points in the testing set correct with a probability of `r lda_validation$overall[["Accuracy"]]`^20, or `r lda_validation$overall[["Accuracy"]]^20`. There are no tuning parameters avaiable for the lda method in caret, which suggests this accelerometer data is not a good fit for the model. This leads me to suspect that that the problem of classifying the technique used during the excercises is not linear.

###Random Forest Performance:
```{r}
rf_validation <- confusionMatrix(rf_pred, clean_validation$classe)
rf_validation
```
Random Forests using the default settings provided by caret performed extremely well with a cross validated accuracy of `r rf_validation$overall[["Accuracy"]]`. We should expect to get all 20 points in the testing set correct with a probability of `r rf_validation$overall[["Accuracy"]]^20`.

### Gradient Boosting Performance:
```{r}
xgbTree_validation <- confusionMatrix(xgbTree_pred, clean_validation$classe)
xgbTree_validation
```
Gradient Boosting using the default settings provided by caret also performed extremely well with a cross validated accuracy of `r xgbTree_validation$overall[["Accuracy"]]`. This is exactly the same accuracy as the random forest model, but a closer look at the confusion matrix shows that they did not make the exact same predictions as each other.

###Support Vector Machines Performance:
```{r}
svm_validation <- confusionMatrix(svm_pred, clean_validation$classe)
svm_validation
```
Support Vector Machines using a Linear Kernel and the default settings provided by caret produced an accuracy of `r svm_validation$overall[["Accuracy"]]`. This means the model performed better than Linear Discriminant Analysis when classifying the exercises, but it is well behind the performance of the random forest and gradient boosting models. This further supports my hypothesis that the problem is non-linear.

###Stacked Model Performance:
```{r}
stacked_validation <- confusionMatrix(stacked_pred, clean_validation$classe)
stacked_validation
```
Stacking the above four models and using a random forest model to make predictions had a cross validation accuracy of `r stacked_validation$overall[["Accuracy"]]`. This exceeds the accuracies of both the random forest and gradient boosting models on their own and means this approach is our strongest contender to be used when we make our predictions on the 20 rows of the testing data frame.

## Final Model Testing

```{r}
lda_test_pred <- predict(lda_mod, clean_testing)
rf_test_pred <- predict(rf_mod, clean_testing)
xgbTree_test_pred <- predict(xgbTree_mod, clean_testing)
svm_test_pred <- predict(svm_mod, clean_testing)

stacked_test <- data.frame(lda_pred = lda_test_pred, rf_pred = rf_test_pred, xgbTree_pred = xgbTree_test_pred, svm_pred = svm_test_pred)
stacked_test_pred <- predict(stacked_mod, stacked_test)

stacked_test_pred
```

