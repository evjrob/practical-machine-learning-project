---
title: "Practical Machine Learning Project"
author: "Everett Robinson"
date: "May 23, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(doParallel)
set.seed(73222)
```

## Data Exploration and Cleaning
We start by importing the data from the provided csv files. I am going to split the training data into a new training subset and a cross validation subset and use the provided pml-testing.csv file for final model testing.
```{r import}
raw_pml_training <- read.csv("pml-training.csv")
in_training <- createDataPartition(y = raw_pml_training$classe, p = 0.8, list = FALSE)
training <- raw_pml_training[in_training,]
validation <- raw_pml_training[-in_training,]
testing <- read.csv("pml-testing.csv")

dim(training)
dim(validation)
```

It looks like there are a large number of rows of data to be used for training and validation, so I will stick with this simple data partition and not use any folding or resampling techniques on the data. There are a large number of predictors however so we will need to investigate these further.

```{r col_names}
names(training)
```

From the source of the data <http://groupware.les.inf.puc-rio.br/har>, we know that six different participants were asked to complete dumbbell bicep curls in five different ways, the first using proper technique and the next four using a variety of incorrect techniques. These differing techniques are identified in the classe variable using the letters A through E. We also know that the main components of the data come from four separate three axis accellerometers. One accelerometer was mounted on the dumbell, and three were located on the particiapants waist, forearm, and upper arm.

The first seven columns appear to be related to data collection rather than actual measurements, and they will need to be removed from each data frame before training is performed:
```{r}
head(training[1:7])
```

```{r}
non_measurement_columns <- 1:ncol(training) <= 7
```


Each accelerometer also recorded 38 different variables during each repitition:
```{r}
length(grep("dumbbell", names(training)))
length(grep("belt", names(training)))
length(grep("forearm", names(training)))
length(grep("_arm", names(training)))
```

These measurements are on various paramters such as roll, pitch, yaw, acceleration, gyros, and magnetic measurements. They are also supplemented by statistics for each measurement such as mean, minimum, maximum, amplitude, variance, standard deviation, skew, and kurtosis.

We hope that the combinations of all of the above will be sufficiently different between the different classe exercise techniques.

```{r}
ggplot(training) + geom_point(aes(x = X, y = roll_dumbbell, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```


```{r}
ggplot(training) + geom_point(aes(x = X, y = pitch_dumbbell, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```


```{r}
ggplot(training) + geom_point(aes(x = X, y = accel_dumbbell_y, colour = classe), alpha = 0.2) + facet_wrap("user_name")
```

It appears there is a lot of variation between how the different participants perform the dumbell curls, which reinforces the descision to remove the user_name column from the training data before training occurs to prevent it from playing a roll in the classification process. There are some visible differences between the classe groups as well, which suggests that the combination of multiple different accelerometer readings for each recorded exercise might be able to distinguish the techniques.

It is unlikely that all 38 columns from each accelerometer will be necessary to properly classify each excersice, and so we will remove or consolidate columns that don't offer a useful amount crutial to classification.

The first step in this process will be to figure out which columns are not well populated with data. After running **View(training)**, the columns containing the summary statistics appear to be very sparse. I suspect that it is the case that only the last row of data in each num_window set was populated with these values, and now they are spread across our training, validation, and testing data sets. Because it would be difficult to correct this issue while still maintaining the separation of the datasets, these columns are prime candidates for removal.

We will start by figuring out how sparse each column actually is:

```{r}
percent_sparse <- function(col) {
  sum((is.na(col) | col == "")/ length(col))
}

sparseness <- apply(training, 2, percent_sparse)

table(sparseness)
names(sparseness[sparseness > 0.95])
```

So it turns out that all of the columns containing summary statistics are nearly 98% empty. We will remove these when cleaning the data:
```{r}
sparse_columns <- sparseness > 0.95
```


The above cleaning steps will need to be performed before the data is ready to be used for training. It is necessary that the same preprocessing steps be performed on each of the training, validation, and testing data frames, so I will put all of the steps in a function that can be run on all three data frames to ensure consistency.
```{r}
clean_data <- function(data) {
  drop_columns <- non_measurement_columns | sparse_columns
  data <- data[,!drop_columns]
  return(data)
}

clean_training <- clean_data(training)
clean_validation <- clean_data(validation)
clean_testing <- clean_data(testing)
```



## Model Building and Selection

We will train several different types of machine learning models before evaluating them both individually and stacked with each other. We will test linear discriminant analysis, random forests, gradient boosting, and support vector machines. The stacked model will be combined using random forests on the predictions of the previous four models. All of these models will be trained using the default parameters provided through the caret package.

```{r}
set.seed(82917)

# We will set up a cluster to parallelize the training steps for performance reasons
cl <- makeCluster(6)
registerDoParallel(cl)
```

```{r linear_discriminant_analysis}
lda_mod <- train(classe ~ ., method = "lda", data = clean_training)
lda_pred <- predict(lda_mod, clean_validation)
```

```{r random_forest}
rf_mod <- train(classe ~ ., method = "parRF", data = clean_training)
rf_pred <-predict(rf_mod, clean_validation)
```

```{r gradient_boosting}
xgbTree_mod <- train(classe ~ ., method = "xgbTree", data = clean_training)
xgbTree_pred <- predict(xgbTree_mod, clean_validation)
```


```{r support_vector_machines}
svm_mod <- train(classe ~ ., method = "svmLinear", data = clean_training)
svm_pred <- predict(svm_mod, clean_validation)
```


```{r stacked}
stacked <- data.frame(lda_pred, rf_pred, xgbTree_pred, svm_pred, classe = clean_validation$classe)
stacked_mod <- train(classe ~ ., method = "rf", data = stacked)
stacked_pred <- predict(stacked_mod, clean_validation)
```

And with all of our models trained we can now see how well each of them performed using the confusionMatrix function in the caret package.

###Linear Discriminant Performance:
```{r}
confusionMatrix(lda_pred, clean_validation$classe)
```

###Random Forest Performance:
```{r}
confusionMatrix(rf_pred, clean_validation$classe)
```

### Gradient Boosting Performance:
```{r}
confusionMatrix(xgbTree_pred, clean_validation$classe)
```

###Support Vector Machines Performance:
```{r}
confusionMatrix(svm_pred, clean_validation$classe)
```

###Stacked Model Performance:
```{r}
confusionMatrix(stacked_pred, clean_validation$classe)
```
